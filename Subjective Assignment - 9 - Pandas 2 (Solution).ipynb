{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link for Datasets\n",
    "(https://drive.google.com/drive/folders/105ftuIwN9kqyPNEEm3E6IM7LqywjyvJa?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q21. Write a pandas program to import three datasheets from a given\n",
    "excel data (coalpublic2013.xlsx ) in to a single dataframe.\n",
    " \n",
    "Note: Structure of three datasheets are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    emp_id   first_name    last_name  hire_date\n",
      "0      100       Steven         King 2003-06-17\n",
      "1      101        Neena      Kochhar 2005-09-21\n",
      "2      102          Lex      De Haan 2001-01-13\n",
      "3      103    Alexander       Hunold 2006-01-03\n",
      "4      104        Bruce        Ernst 2007-05-21\n",
      "5      105        David       Austin 2005-06-25\n",
      "6      106        Valli    Pataballa 2006-02-05\n",
      "7      107        Diana      Lorentz 2007-02-07\n",
      "8      108        Nancy    Greenberg 2002-08-17\n",
      "9      109       Daniel       Faviet 2002-08-16\n",
      "10     110         John         Chen 2005-09-28\n",
      "11     111       Ismael      Sciarra 2005-09-30\n",
      "12     112  Jose Manuel        Urman 2006-03-07\n",
      "13     113         Luis         Popp 2007-12-07\n",
      "14     114          Den     Raphaely 2002-12-07\n",
      "15     115    Alexander         Khoo 2003-05-18\n",
      "16     116       Shelli        Baida 2005-12-24\n",
      "17     117        Sigal       Tobias 2005-07-24\n",
      "18     118          Guy       Himuro 2006-11-15\n",
      "19     119        Karen   Colmenares 2007-08-10\n",
      "0      120      Matthew        Weiss 2004-07-18\n",
      "1      121         Adam        Fripp 2005-04-10\n",
      "2      122        Payam     Kaufling 2003-05-01\n",
      "3      123       Shanta      Vollman 2005-10-10\n",
      "4      124        Kevin      Mourgos 2007-11-16\n",
      "5      125        Julia        Nayer 2005-07-16\n",
      "6      126        Irene  Mikkilineni 2006-09-28\n",
      "7      127        James       Landry 2007-01-14\n",
      "8      128       Steven       Markle 2008-03-08\n",
      "9      129        Laura       Bissot 2005-08-20\n",
      "10     130        Mozhe     Atkinson 2005-10-30\n",
      "11     131        James       Marlow 2005-02-16\n",
      "12     132           TJ        Olson 2007-04-10\n",
      "13     133        Jason       Mallin 2004-06-14\n",
      "14     134      Michael       Rogers 2006-08-26\n",
      "15     135           Ki          Gee 2007-12-12\n",
      "16     136        Hazel   Philtanker 2008-02-06\n",
      "17     137       Renske       Ladwig 2003-07-14\n",
      "18     138      Stephen       Stiles 2005-10-26\n",
      "0      141       Trenna         Rajs 2003-10-17\n",
      "1      142       Curtis       Davies 2005-01-29\n",
      "2      143      Randall        Matos 2006-03-15\n",
      "3      144        Peter       Vargas 2006-07-09\n",
      "4      145         John      Russell 2004-10-01\n",
      "5      146        Karen     Partners 2005-01-05\n",
      "6      147      Alberto    Errazuriz 2005-03-10\n",
      "7      148       Gerald    Cambrault 2007-10-15\n",
      "8      149        Eleni      Zlotkey 2008-01-29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df1 = pd.read_excel('C:/Users/User/Downloads/employee.xlsx',sheet_name=0)\n",
    "df2 = pd.read_excel('C:/Users/User/Downloads/employee.xlsx',sheet_name=1) \n",
    "df3 = pd.read_excel('C:/Users/User/Downloads/employee.xlsx',sheet_name=2) \n",
    "df = pd.concat([df1, df2, df3]) \n",
    "print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 22. Write a pandas program to import three datasheets from a given\n",
    "excel data (employee.xlsx ) into a single data frame and export the\n",
    "result into new Excel file.\n",
    " \n",
    "Note: Structure of three datasheets are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df1 = pd.read_excel('C:/Users/User/Downloads/employee.xlsx',sheet_name=1) \n",
    "df3 = pd.read_excel('C:/Users/User/Downloads/employee.xlsx',sheet_name=2) \n",
    "df = pd.concat([df1, df2, df3]) \n",
    "df.to_excel('C:/Users/User/Downloads/output.xlsx', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q23. Write a pandas program to create the Pivot table with multiple\n",
    "indexes from the data set of the titanic.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Unnamed: 15  adult_male  alone      fare  parch  pclass  sibsp  \\\n",
      "sex    age                                                                     \n",
      "female 0.75           0.0         0.0    0.0   38.5166      2       6      4   \n",
      "       1.00           0.0         0.0    0.0   26.8750      3       6      1   \n",
      "       2.00           0.0         0.0    0.0  259.4750      9      15      9   \n",
      "       3.00           0.0         0.0    0.0   62.6542      3       5      4   \n",
      "       4.00           0.0         0.0    0.0  114.1417      6      13      4   \n",
      "...                   ...         ...    ...       ...    ...     ...    ...   \n",
      "male   70.00          0.0         2.0    1.0   81.5000      1       3      1   \n",
      "       70.50          0.0         1.0    1.0    7.7500      0       3      0   \n",
      "       71.00          0.0         2.0    2.0   84.1584      0       2      0   \n",
      "       74.00          0.0         1.0    1.0    7.7750      0       3      0   \n",
      "       80.00          0.0         1.0    1.0   30.0000      0       1      0   \n",
      "\n",
      "              survived  \n",
      "sex    age              \n",
      "female 0.75          2  \n",
      "       1.00          2  \n",
      "       2.00          2  \n",
      "       3.00          1  \n",
      "       4.00          5  \n",
      "...                ...  \n",
      "male   70.00         0  \n",
      "       70.50         0  \n",
      "       71.00         0  \n",
      "       74.00         0  \n",
      "       80.00         1  \n",
      "\n",
      "[145 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result = pd.pivot_table(df, index = [\"sex\",\"age\"], aggfunc=np.sum) \n",
    "print(result) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q24. Write a Pandas program to create the Pivot table and find survival\n",
    "rate by gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        survived\n",
      "sex             \n",
      "female  0.742038\n",
      "male    0.188908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result=df.groupby('sex')[['survived']].mean() \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q25. Write a pandas program to make partition each of the passengers\n",
    "into 4 categories based on their age.\n",
    " \n",
    "Note: Age categories- (0, 10), (10, 30), (30, 60), (60, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      (10.0, 30.0]\n",
      "1      (30.0, 60.0]\n",
      "2      (10.0, 30.0]\n",
      "3      (30.0, 60.0]\n",
      "4      (30.0, 60.0]\n",
      "           ...     \n",
      "886    (10.0, 30.0]\n",
      "887    (10.0, 30.0]\n",
      "888             NaN\n",
      "889    (10.0, 30.0]\n",
      "890    (30.0, 60.0]\n",
      "Name: age, Length: 891, dtype: category\n",
      "Categories (4, interval[int64]): [(0, 10] < (10, 30] < (30, 60] < (60, 80]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result = pd.cut(df['age'], [0, 10, 30, 60, 80]) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q26. Write a pandas program to create the Pivot table and find survival\n",
    "rate by the gender, age of the different categories of various\n",
    "classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class               First    Second     Third\n",
      "sex    age                                   \n",
      "female (0, 20]   0.928571  1.000000  0.510638\n",
      "       (20, 55]  0.968750  0.912281  0.407407\n",
      "male   (0, 20]   0.571429  0.526316  0.197368\n",
      "       (20, 55]  0.440000  0.054054  0.134503\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "age = pd.cut(df['age'], [0, 20, 55]) \n",
    "result = df.pivot_table('survived', index=['sex', age], columns='class') \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q27. Write a pandas program to create the Pivot table and calculate\n",
    "number of women and men were in a particular cabin class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 15       adult_male            age          alive  ... parch  \\\n",
      "pclass           1  2  3          1    2    3    1   2    3     1  ...     3   \n",
      "sex                                                                ...         \n",
      "female           0  0  0         94   76  144   85  74  102    94  ...   144   \n",
      "male             0  0  0        122  108  347  101  99  253   122  ...   347   \n",
      "\n",
      "       sibsp           survived            who            \n",
      "pclass     1    2    3        1    2    3    1    2    3  \n",
      "sex                                                       \n",
      "female    94   76  144       94   76  144   94   76  144  \n",
      "male     122  108  347      122  108  347  122  108  347  \n",
      "\n",
      "[2 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result = df.pivot_table(index=['sex'], columns=['pclass'], aggfunc='count') \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q28. Write a pandas program to create the Pivot table and separate\n",
    "the gender according to whether they travelled alone or not to get\n",
    "the probability of survival\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class            First    Second     Third\n",
      "sex    alone                              \n",
      "female False  0.966667  0.931818  0.416667\n",
      "       True   0.970588  0.906250  0.616667\n",
      "male   False  0.425532  0.277778  0.180723\n",
      "       True   0.333333  0.097222  0.121212\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result = df.pivot_table( 'survived' , [ 'sex' , 'alone' ] , 'class' ) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q29. Write a pandas program to create the Pivot table and find the\n",
    "probability of survival by class, gender, solo boarding, and the port\n",
    "of embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embark_town  Cherbourg                     Queenstown                   \\\n",
      "class            First    Second     Third      First Second     Third   \n",
      "sex    alone                                                             \n",
      "female False  1.000000  1.000000  0.611111        1.0    NaN  0.625000   \n",
      "       True   0.944444  1.000000  0.800000        NaN    1.0  0.760000   \n",
      "male   False  0.473684  0.166667  0.500000        0.0    NaN  0.100000   \n",
      "       True   0.347826  0.250000  0.151515        NaN    0.0  0.068966   \n",
      "\n",
      "embark_town  Southampton                      \n",
      "class              First    Second     Third  \n",
      "sex    alone                                  \n",
      "female False    0.941176  0.923077  0.327586  \n",
      "       True     1.000000  0.892857  0.466667  \n",
      "male   False    0.407407  0.300000  0.142857  \n",
      "       True     0.326923  0.089552  0.123762  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    " \n",
    "df = pd.read_csv('C:/Users/User/Downloads/titanic.csv') \n",
    "result = df.pivot_table('survived', ['sex' , 'alone' ], [ 'embark_town', 'class' ]) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q30. Write a pandas program to get current date, oldest date and\n",
    "number of days between Current date and the oldest date of Ufo\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This line in every code is giving error or there is some problem with the ufo_sighting_data.csv file:\n",
    "\n",
    "df['Date_time'] = df['Date_time'].astype('datetime64[ns]') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe:\n",
      "          Date_time                  city state/province country UFO_shape  \\\n",
      "0  10-10-1949 20:30            san marcos             tx      us  cylinder   \n",
      "1  10-10-1949 21:00          lackland afb             tx     NaN     light   \n",
      "2  10-10-1955 17:00  chester (uk/england)            NaN      gb    circle   \n",
      "3  10-10-1956 21:00                  edna             tx      us    circle   \n",
      "4  10-10-1960 20:00               kaneohe             hi      us     light   \n",
      "\n",
      "  length_of_encounter_seconds described_duration_of_encounter  \\\n",
      "0                        2700                      45 minutes   \n",
      "1                        7200                         1-2 hrs   \n",
      "2                          20                      20 seconds   \n",
      "3                          20                        1/2 hour   \n",
      "4                         900                      15 minutes   \n",
      "\n",
      "                                         description date_documented  \\\n",
      "0  This event took place in early fall around 194...       4/27/2004   \n",
      "1  1949 Lackland AFB&#44 TX.  Lights racing acros...      12/16/2005   \n",
      "2  Green/Orange circular disc over Chester&#44 En...       1/21/2008   \n",
      "3  My older brother and twin sister were leaving ...       1/17/2004   \n",
      "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...       1/22/2004   \n",
      "\n",
      "     latitude   longitude  \n",
      "0  29.8830556  -97.941111  \n",
      "1    29.38421  -98.581082  \n",
      "2        53.2   -2.916667  \n",
      "3  28.9783333  -96.645833  \n",
      "4  21.4180556 -157.803611  \n",
      "\n",
      "Current date of Ufo dataset:\n",
      "9/30/2013 22:30\n",
      "\n",
      "Oldest date of Ufo dataset:\n",
      "01-01-1944 12:00\n",
      "\n",
      "Number of days between Current date and oldest date of Ufo dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (5,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8b1745f2bc3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nNumber of days between Current date and oldest date of Ufo dataset:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd \n",
    "df = pd.read_csv(r'C:/Users/User/Downloads/ufo_sighting_data.csv') \n",
    "#df['Date_time'] = df['Date_time'].astype('datetime64[ns]') \n",
    "print(\"Original Dataframe:\") \n",
    "print(df.head()) \n",
    "print(\"\\nCurrent date of Ufo dataset:\") \n",
    "print(df.Date_time.max()) \n",
    "print(\"\\nOldest date of Ufo dataset:\") \n",
    "print(df.Date_time.min()) \n",
    "print(\"\\nNumber of days between Current date and oldest date of Ufo dataset:\") \n",
    "print((df.Date_time.max() - df.Date_time.min()).days) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q31. Write a pandas program to get all sighting days of the\n",
    "unidentified flying object (ufo) between 1950-10-10 and 1960-10-\n",
    "10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe:\n",
      "          Date_time                  city state/province country UFO_shape  \\\n",
      "0  10-10-1949 20:30            san marcos             tx      us  cylinder   \n",
      "1  10-10-1949 21:00          lackland afb             tx     NaN     light   \n",
      "2  10-10-1955 17:00  chester (uk/england)            NaN      gb    circle   \n",
      "3  10-10-1956 21:00                  edna             tx      us    circle   \n",
      "4  10-10-1960 20:00               kaneohe             hi      us     light   \n",
      "\n",
      "  length_of_encounter_seconds described_duration_of_encounter  \\\n",
      "0                        2700                      45 minutes   \n",
      "1                        7200                         1-2 hrs   \n",
      "2                          20                      20 seconds   \n",
      "3                          20                        1/2 hour   \n",
      "4                         900                      15 minutes   \n",
      "\n",
      "                                         description date_documented  \\\n",
      "0  This event took place in early fall around 194...       4/27/2004   \n",
      "1  1949 Lackland AFB&#44 TX.  Lights racing acros...      12/16/2005   \n",
      "2  Green/Orange circular disc over Chester&#44 En...       1/21/2008   \n",
      "3  My older brother and twin sister were leaving ...       1/17/2004   \n",
      "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...       1/22/2004   \n",
      "\n",
      "     latitude   longitude  \n",
      "0  29.8830556  -97.941111  \n",
      "1    29.38421  -98.581082  \n",
      "2        53.2   -2.916667  \n",
      "3  28.9783333  -96.645833  \n",
      "4  21.4180556 -157.803611  \n",
      "\n",
      "Sighting days of the unidentified flying object (ufo) between 1949-10-10 and 1960-10-10:\n",
      "Empty DataFrame\n",
      "Columns: [Date_time, city, state/province, country, UFO_shape, length_of_encounter_seconds, described_duration_of_encounter, description, date_documented, latitude, longitude]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(r'C:/Users/User/Downloads/ufo_sighting_data.csv')\n",
    "#df['Date_time'] = df['Date_time'].astype('datetime64[ns]') \n",
    "print(\"Original Dataframe:\") \n",
    "print(df.head()) \n",
    "print(\"\\nSighting days of the unidentified flying object (ufo) between 1949-10-10 and 1960-10-10:\")\n",
    "selected_period = df[(df['Date_time'] >= '1950-01-01 00:00:00') & (df['Date_time'] <= '1960-12-31 23:59:59')] \n",
    "print(selected_period) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q32. Write a Pandas program to extract the year, month, day, hour,\n",
    "minute, second, and weekday from unidentified flying object (UFO)\n",
    "reporting date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q33. Write a pandas program to count year-country wise frequency of\n",
    "reporting dates of the unidentified flying object(UFO).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q34. Write a pandas program to get the difference (in days) between\n",
    "documented date and reporting date of unidentified flying object\n",
    "(UFO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q35. Write a pandas program to generate sequences of fixedfrequency dates and time spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly frequency:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',\n",
      "               '2018-01-01 02:00:00', '2018-01-01 03:00:00',\n",
      "               '2018-01-01 04:00:00', '2018-01-01 05:00:00',\n",
      "               '2018-01-01 06:00:00', '2018-01-01 07:00:00',\n",
      "               '2018-01-01 08:00:00', '2018-01-01 09:00:00',\n",
      "               '2018-01-01 10:00:00', '2018-01-01 11:00:00'],\n",
      "              dtype='datetime64[ns]', freq='H')\n",
      "\n",
      "Minutely frequency:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 00:01:00',\n",
      "               '2018-01-01 00:02:00', '2018-01-01 00:03:00',\n",
      "               '2018-01-01 00:04:00', '2018-01-01 00:05:00',\n",
      "               '2018-01-01 00:06:00', '2018-01-01 00:07:00',\n",
      "               '2018-01-01 00:08:00', '2018-01-01 00:09:00',\n",
      "               '2018-01-01 00:10:00', '2018-01-01 00:11:00'],\n",
      "              dtype='datetime64[ns]', freq='T')\n",
      "\n",
      "Secondly frequency:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 00:00:01',\n",
      "               '2018-01-01 00:00:02', '2018-01-01 00:00:03',\n",
      "               '2018-01-01 00:00:04', '2018-01-01 00:00:05',\n",
      "               '2018-01-01 00:00:06', '2018-01-01 00:00:07',\n",
      "               '2018-01-01 00:00:08', '2018-01-01 00:00:09',\n",
      "               '2018-01-01 00:00:10', '2018-01-01 00:00:11'],\n",
      "              dtype='datetime64[ns]', freq='S')\n",
      "nMultiple Hourly frequency:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 02:00:00',\n",
      "               '2018-01-01 04:00:00', '2018-01-01 06:00:00',\n",
      "               '2018-01-01 08:00:00', '2018-01-01 10:00:00',\n",
      "               '2018-01-01 12:00:00', '2018-01-01 14:00:00',\n",
      "               '2018-01-01 16:00:00', '2018-01-01 18:00:00',\n",
      "               '2018-01-01 20:00:00', '2018-01-01 22:00:00'],\n",
      "              dtype='datetime64[ns]', freq='2H')\n",
      "\n",
      "Multiple Minutely frequency:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 00:05:00',\n",
      "               '2018-01-01 00:10:00', '2018-01-01 00:15:00',\n",
      "               '2018-01-01 00:20:00', '2018-01-01 00:25:00',\n",
      "               '2018-01-01 00:30:00', '2018-01-01 00:35:00',\n",
      "               '2018-01-01 00:40:00', '2018-01-01 00:45:00',\n",
      "               '2018-01-01 00:50:00', '2018-01-01 00:55:00'],\n",
      "              dtype='datetime64[ns]', freq='5T')\n",
      "\n",
      "Multiple Secondly frequency:\n",
      "DatetimeIndex(['2018-03-30', '2018-06-29', '2018-09-28', '2018-12-31',\n",
      "               '2019-03-29', '2019-06-28', '2019-09-30', '2019-12-31',\n",
      "               '2020-03-31', '2020-06-30', '2020-09-30', '2020-12-31'],\n",
      "              dtype='datetime64[ns]', freq='BQ-DEC')\n",
      "\n",
      "Weekly frequency:\n",
      "DatetimeIndex(['2018-01-07', '2018-01-14', '2018-01-21', '2018-01-28',\n",
      "               '2018-02-04', '2018-02-11', '2018-02-18', '2018-02-25',\n",
      "               '2018-03-04', '2018-03-11', '2018-03-18', '2018-03-25'],\n",
      "              dtype='datetime64[ns]', freq='W-SUN')\n",
      "\n",
      "Combine together day and intraday offsets-1:\n",
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 02:20:00',\n",
      "               '2018-01-01 04:40:00', '2018-01-01 07:00:00',\n",
      "               '2018-01-01 09:20:00', '2018-01-01 11:40:00',\n",
      "               '2018-01-01 14:00:00', '2018-01-01 16:20:00',\n",
      "               '2018-01-01 18:40:00', '2018-01-01 21:00:00',\n",
      "               '2018-01-01 23:20:00', '2018-01-02 01:40:00'],\n",
      "              dtype='datetime64[ns]', freq='140T')\n",
      "\n",
      "Combine together day and intraday offsets-2:\n",
      "DatetimeIndex([       '2018-01-01 00:00:00', '2018-01-02 00:00:00.000010',\n",
      "               '2018-01-03 00:00:00.000020', '2018-01-04 00:00:00.000030',\n",
      "               '2018-01-05 00:00:00.000040', '2018-01-06 00:00:00.000050',\n",
      "               '2018-01-07 00:00:00.000060', '2018-01-08 00:00:00.000070',\n",
      "               '2018-01-09 00:00:00.000080', '2018-01-10 00:00:00.000090',\n",
      "               '2018-01-11 00:00:00.000100', '2018-01-12 00:00:00.000110'],\n",
      "              dtype='datetime64[ns]', freq='86400000010U')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    " \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='H') \n",
    "print(\"Hourly frequency:\") \n",
    "print(dtr) \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='min') \n",
    "print(\"\\nMinutely frequency:\")\n",
    "print(dtr)\n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='S')\n",
    "print(\"\\nSecondly frequency:\")\n",
    "print(dtr) \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='2H')\n",
    "print(\"nMultiple Hourly frequency:\") \n",
    "print(dtr) \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='5min') \n",
    "print(\"\\nMultiple Minutely frequency:\") \n",
    "print(dtr)\n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='BQ') \n",
    "print(\"\\nMultiple Secondly frequency:\") \n",
    "print(dtr) \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='w')\n",
    "print(\"\\nWeekly frequency:\") \n",
    "print(dtr)\n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='2h20min') \n",
    "print(\"\\nCombine together day and intraday offsets-1:\")\n",
    "print(dtr) \n",
    "dtr = pd.date_range('2018-01-01', periods=12, freq='1D10U') \n",
    "print(\"\\nCombine together day and intraday offsets-2:\") \n",
    "print(dtr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q36. Write a pandas program to manipulate and convert date times\n",
    "with timezone information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-01-01 00:00:00+00:00', '2018-01-01 01:00:00+00:00',\n",
      "               '2018-01-01 02:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', freq='H')\n",
      "\n",
      "From UTC to America/Los_Angeles:\n",
      "DatetimeIndex(['2017-12-31 16:00:00-08:00', '2017-12-31 17:00:00-08:00',\n",
      "               '2017-12-31 18:00:00-08:00'],\n",
      "              dtype='datetime64[ns, America/Los_Angeles]', freq='H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "dtt = pd.date_range('2018-01-01', periods=3, freq='H')\n",
    "dtt = dtt.tz_localize('UTC') \n",
    "print(dtt)\n",
    "print(\"\\nFrom UTC to America/Los_Angeles:\") \n",
    "dtt = dtt.tz_convert('America/Los_Angeles') \n",
    "print(dtt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q37. Write a pandas program to create the graphical analysis of UFO\n",
    "(unidentified flying object) Sightings year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q38. Write a pandas program to create a comparison of the top 10\n",
    "years in which the (UFO) was sighted VS each Month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q39. Write a pandas program to create a heatmap (rectangular data as\n",
    "a colour-encoded matrix) for comparison of top 10 years in\n",
    "which (UFO ) was sighted VS each Month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q40. Write a pandas program to create a Timewheel of Hour VS Year\n",
    "comparison of the top 10 years in which the (UFO) was sighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
